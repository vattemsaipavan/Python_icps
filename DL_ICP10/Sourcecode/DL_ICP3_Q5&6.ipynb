{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL_ICP3_Q5&6.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GRCvfqdEQ5Pa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "78ebd92c-33ff-48fd-8db1-e8bbd07d5c6d"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "import pandas as pd\n",
        "from sklearn import preprocessing\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Embedding, Flatten\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TJoNyVFEQ5wD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "2cc40f8a-9c4d-4419-ebe3-d7827d73b601"
      },
      "source": [
        "cats = ['alt.atheism', 'sci.space']\n",
        "df = fetch_20newsgroups(subset='train', shuffle=True, categories=cats)\n",
        "sentences=df.data\n",
        "y=df.target"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading 20news dataset. This may take a few minutes.\n",
            "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WoDZ9OX_REeG",
        "colab": {}
      },
      "source": [
        "le = preprocessing.LabelEncoder()\n",
        "y = le.fit_transform(y)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8wfA7zlbRGva",
        "colab": {}
      },
      "source": [
        "#tokenizing data\n",
        "tokenizer = Tokenizer(num_words=2000)\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "max_review_len = max([len(s.split()) for s in sentences])\n",
        "vocab_size = len(tokenizer.word_index)+1\n",
        "X_train, X_test, y_train, y_test = train_test_split(sentences, y, test_size=0.25, random_state=1000)\n",
        "X_train_tokens = tokenizer.texts_to_sequences(X_train)\n",
        "X_test_tokens = tokenizer.texts_to_sequences(X_test)\n",
        "padded_train = pad_sequences(X_train_tokens,maxlen=max_review_len)\n",
        "paded_test = pad_sequences(X_test_tokens,maxlen=max_review_len)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UcveeysNRLLR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "outputId": "4e021484-16da-4327-9799-52bb6a79e69f"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 50, input_length=max_review_len))\n",
        "model.add(Flatten())\n",
        "model.add(layers.Dense(300, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid')) #changing number of neuron to 2 as we have only two labels Pos and Neg\n",
        "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['acc'])\n",
        "history=model.fit(padded_train,y_train, epochs=5, verbose=True, validation_data=(paded_test,y_test), batch_size=256)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 804 samples, validate on 269 samples\n",
            "Epoch 1/5\n",
            "804/804 [==============================] - 22s 27ms/step - loss: 9.6456 - acc: 0.5124 - val_loss: 13.3701 - val_acc: 0.4089\n",
            "Epoch 2/5\n",
            "804/804 [==============================] - 20s 24ms/step - loss: 7.0877 - acc: 0.5299 - val_loss: 4.9299 - val_acc: 0.5911\n",
            "Epoch 3/5\n",
            "804/804 [==============================] - 19s 24ms/step - loss: 4.0196 - acc: 0.5249 - val_loss: 5.3581 - val_acc: 0.4089\n",
            "Epoch 4/5\n",
            "804/804 [==============================] - 19s 24ms/step - loss: 3.0354 - acc: 0.5485 - val_loss: 4.1898 - val_acc: 0.5911\n",
            "Epoch 5/5\n",
            "804/804 [==============================] - 19s 24ms/step - loss: 2.6856 - acc: 0.5187 - val_loss: 2.1399 - val_acc: 0.4089\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lmg18xGVBOHV"
      },
      "source": [
        "Checking Sample Data and Predicted Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "j3GsqTNXXD09",
        "colab": {}
      },
      "source": [
        "x=model.predict_classes(paded_test[[1],:])"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "piR8Kn3XAghW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d9f5a526-ecc5-4b65-8424-0b821b5cdd61"
      },
      "source": [
        "print(\"Actual Prediction\",y_test[1],\"Predicted Prediction\",x)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Actual Prediction 0 Predicted Prediction [[0]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}