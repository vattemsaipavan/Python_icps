{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z5ziio4bPwo3"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 150
    },
    "colab_type": "code",
    "id": "9O02CYV6QbBa",
    "outputId": "6733e518-acbc-4e30-d9f1-b54f031ad1cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0  type                                             review label  \\\n",
      "0           0  test  Once again Mr. Costner has dragged out a movie...   neg   \n",
      "1           1  test  This is an example of why the majority of acti...   neg   \n",
      "2           2  test  First of all I hate those moronic rappers, who...   neg   \n",
      "3           3  test  Not even the Beatles could write songs everyon...   neg   \n",
      "4           4  test  Brass pictures (movies is not a fitting word f...   neg   \n",
      "\n",
      "          file  \n",
      "0      0_2.txt  \n",
      "1  10000_4.txt  \n",
      "2  10001_1.txt  \n",
      "3  10002_3.txt  \n",
      "4  10003_3.txt  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('imdb_master.csv',encoding='latin-1')\n",
    "print(df.head())\n",
    "df = df[df['label']!='unsup'] #Dropping Unnecessary labelfrom dataset\n",
    "sentences = df['review'].values\n",
    "y = df['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L0Hrpn9EQdz_"
   },
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "y = le.fit_transform(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kob4QHmiQwWB"
   },
   "source": [
    "Tokenizing data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0rqzAff_QgCG"
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=2000)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "max_review_len = max([len(s.split()) for s in sentences])\n",
    "vocab_size = len(tokenizer.word_index)+1\n",
    "X_train, X_test, y_train, y_test = train_test_split(sentences, y, test_size=0.25, random_state=1000)\n",
    "X_train_tokens = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_tokens = tokenizer.texts_to_sequences(X_test)\n",
    "padded_train = pad_sequences(X_train_tokens,maxlen=max_review_len)\n",
    "paded_test = pad_sequences(X_test_tokens,maxlen=max_review_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 253
    },
    "colab_type": "code",
    "id": "uB9nug5lQit-",
    "outputId": "5cf2fa91-fc05-4b22-f274-7a2e4a4882a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "147/147 [==============================] - 246s 2s/step - loss: 0.6245 - acc: 0.7173 - val_loss: 0.2998 - val_acc: 0.8761\n",
      "Epoch 2/5\n",
      "147/147 [==============================] - 221s 2s/step - loss: 0.2534 - acc: 0.8978 - val_loss: 0.2800 - val_acc: 0.8814\n",
      "Epoch 3/5\n",
      "147/147 [==============================] - 265s 2s/step - loss: 0.1858 - acc: 0.9314 - val_loss: 0.3062 - val_acc: 0.8729\n",
      "Epoch 4/5\n",
      "147/147 [==============================] - 269s 2s/step - loss: 0.1170 - acc: 0.9646 - val_loss: 0.3243 - val_acc: 0.8736\n",
      "Epoch 5/5\n",
      "147/147 [==============================] - 267s 2s/step - loss: 0.0586 - acc: 0.9888 - val_loss: 0.3750 - val_acc: 0.8678\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 50, input_length=max_review_len))\n",
    "model.add(Flatten())\n",
    "model.add(layers.Dense(300, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid')) #changing number of neuron to 2 as we have only two labels Pos and Neg\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['acc'])\n",
    "history=model.fit(padded_train,y_train, epochs=5, verbose=True, validation_data=(paded_test,y_test), batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "NK5rGukoQq_q",
    "outputId": "146a15dc-b0f7-4185-bb96-bf35a8665399"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 29s 75ms/step - loss: 0.3750 - acc: 0.8678\n",
      "0.8678399920463562\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(paded_test, y_test)\n",
    "print(test_acc)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "DL_ICP3_Q2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
